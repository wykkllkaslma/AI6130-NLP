judge.py 评估函数 在answer_eva.py中调用

answer_eva.py怎么用？
对于传统的rretrieve评估方法需要构建数据集，我们这里取巧，只用LLM进行评估
（其实也可以加人工打分？但是是不是有点复杂）

# 1) 配置环境
# export DEEPSEEK_API_KEY=sk-ea1c109801b24fd5aa96b3d82d4193cb
# # 可选：设置不同模型
# export JUDGE_MODEL=deepseek-chat
# export GEN_MODEL=deepseek-chat
#
# # 2) 直接传问题
# python answer_eva.py --queries "First-line therapy for CAP in adults?" "Contraindications of isotretinoin?"
#
# # 3) 从文件读取
# printf "First-line therapy for CAP in adults?\nContraindications of isotretinoin?\n" > queries.txt
# python answer_eva.py --queries_file queries.txt --out judge_report.jsonl --repeats 2

我们对rag检索结果answer进行评估，通过多个维度，输出一个分数，并且保存各项指标结果到judge_report中
queries.txt是保存的提问问题，可以自定义

可以考虑在更换不同LLM base model之后都进行一个评估，就能消融什么base LLM在这个medical rag setting下效果最好
可以在rag向量库由少到多过程中进行评估，消融数据集大小对输出的影响
可以更换评估LLM base model，取平均拿到最终评估分数

